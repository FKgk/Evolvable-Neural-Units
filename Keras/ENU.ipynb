{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "from keras import backend as K\n",
    "from keras import activations\n",
    "from keras import initializers\n",
    "from keras import regularizers\n",
    "from keras import constraints\n",
    "\n",
    "from keras.engine.base_layer import Layer\n",
    "# from keras.engine.base_layer import disable_tracking\n",
    "from keras.engine.base_layer import InputSpec\n",
    "from keras.utils.generic_utils import has_arg\n",
    "from keras.utils.generic_utils import to_list\n",
    "\n",
    "# Legacy support.\n",
    "from keras.legacy.layers import Recurrent\n",
    "from keras.legacy import interfaces\n",
    "\n",
    "from keras.layers.recurrent import RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# units 추가해야 함\n",
    "class ENUCell(Layer):\n",
    "    def __init__(self, units,\n",
    "                 reset_activation='sigmoid',\n",
    "                 update_activation='sigmoid',\n",
    "                 cell_activation='tanh',\n",
    "                 use_bias=True,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 memory_state_kernel_initializer='orthogonal',\n",
    "                 output_kernel_initializer='orthogonal',\n",
    "                 bias_initializer='zeros',\n",
    "                 kernel_regularizer=None,\n",
    "                 memory_state_kernel_regularizer=None,\n",
    "                 output_kernel_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 memory_state_kernel_constraint=None,\n",
    "                 output_kernel_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 dropout=0.,\n",
    "                 memory_state_dropout=0.,\n",
    "                 output_dropout=0.,\n",
    "                 implementation=2,\n",
    "                 memory_state_after=False,\n",
    "                 output_after=False,\n",
    "                 **kwargs):\n",
    "        super(ENUCell, self).__init__(**kwargs)\n",
    "        self.output_size = units # memory size\n",
    "        self.state_size =  units# output size <- 문제\n",
    "        self.units = 4\n",
    "        self.use_bias = use_bias\n",
    "        \n",
    "        self.reset_gate_activation = activations.get(reset_activation)\n",
    "        self.update_gate_activation = activations.get(update_activation)\n",
    "        self.cell_gate_activation = activations.get(cell_activation)\n",
    "\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.memory_state_kernel_initializer = initializers.get(memory_state_kernel_initializer)\n",
    "        self.output_kernel_initializer = initializers.get(output_kernel_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "        \n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.memory_state_kernel_regularizer = regularizers.get(memory_state_kernel_regularizer)\n",
    "        self.output_kernel_regularizer = regularizers.get(output_kernel_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.memory_state_kernel_constraint = constraints.get(memory_state_kernel_constraint)\n",
    "        self.output_kernel_constraint = constraints.get(output_kernel_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "\n",
    "        self.dropout = min(1., max(0., dropout))\n",
    "        self.memory_state_dropout = min(1., max(0., memory_state_dropout))\n",
    "        self.output_dropout = min(1., max(0., memory_state_dropout))\n",
    "        \n",
    "        self.implementation = implementation\n",
    "        self.memory_state_after = memory_state_after\n",
    "        self.output_after = output_after\n",
    "\n",
    "        self._dropout_mask = None\n",
    "        self._memory_state_dropout_mask = None\n",
    "        self._output_dropout_mask = None\n",
    "        \n",
    "        self.h = None\n",
    "\n",
    "    #-----------------------------------------------------------------------\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "        z r h만 필요\n",
    "        \"\"\"\n",
    "        input_dim = input_shape[-1]\n",
    "\n",
    "#         if isinstance(self.recurrent_initializer, initializers.Identity):\n",
    "#             def recurrent_identity(shape, gain=1., dtype=None):\n",
    "#                 del dtype\n",
    "#                 return gain * np.concatenate(\n",
    "#                     [np.identity(shape[0])] * (shape[1] // shape[0]), axis=1)\n",
    "\n",
    "#             self.recurrent_initializer = recurrent_identity\n",
    "        \n",
    "\n",
    "        self.kernel = self.add_weight(shape=(input_dim,  self.units * 3),\n",
    "                                      name='kernel',\n",
    "                                      initializer=self.kernel_initializer,\n",
    "                                      regularizer=self.kernel_regularizer,\n",
    "                                      constraint=self.kernel_constraint)\n",
    "        \n",
    "        self.memory_state_kernel = self.add_weight(\n",
    "            shape=(self.units, self.units * 3 + self.output_size),\n",
    "            name='memory_state_kernel',\n",
    "            initializer=self.memory_state_kernel_initializer,\n",
    "            regularizer=self.memory_state_kernel_regularizer,\n",
    "            constraint=self.memory_state_kernel_constraint)\n",
    "\n",
    "        self.output_kernel = self.add_weight(\n",
    "            shape=(self.output_size, self.units * 3),\n",
    "            name='output_kernel',\n",
    "            initializer=self.output_kernel_initializer,\n",
    "            regularizer=self.output_kernel_regularizer,\n",
    "            constraint=self.output_kernel_constraint)\n",
    "        \n",
    "        if self.use_bias: # 이해 필요\n",
    "            if not self.memory_state_after and not self.output_after:\n",
    "                bias_shape = (self.units * 3,)\n",
    "            elif not self.memory_state_after or not self.output_after:\n",
    "                bias_shape = (2, self.units * 3)\n",
    "            else:\n",
    "                bias_shape = (3, self.units * 3)\n",
    "            self.bias = self.add_weight(shape=bias_shape,\n",
    "                                        name='bias',\n",
    "                                        initializer=self.bias_initializer,\n",
    "                                        regularizer=self.bias_regularizer,\n",
    "                                        constraint=self.bias_constraint)\n",
    "            \n",
    "            # NOTE: need to flatten, since slicing in CNTK gives 2D array\n",
    "            if not self.memory_state_after and not self.output_after:\n",
    "                self.input_bias, self.memory_state_bias, self.output_bias = self.bias, None, None\n",
    "            elif not self.memory_state_after:\n",
    "                self.input_bias =  K.flatten(self.bias[0])\n",
    "                self.memory_state_bias =  K.flatten(self.bias[1])\n",
    "                self.output_bias = None\n",
    "            elif not self.output_after:\n",
    "                self.input_bias =  K.flatten(self.bias[0])\n",
    "                self.memory_state_bias = None\n",
    "                self.output_bias = K.flatten(self.bias[1])\n",
    "            else:\n",
    "                self.input_bias = K.flatten(self.bias[0])\n",
    "                self.memory_state_bias = K.flatten(self.bias[1])\n",
    "                self.output_bias = K.flatten(self.bias[2])\n",
    "\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "        # update gate\n",
    "        self.kernel_z = self.kernel[:, :self.units]\n",
    "        self.memory_state_kernel_z = self.memory_state_kernel[:, :self.units]\n",
    "        self.output_kernel_z = self.output_kernel[:, :self.units]\n",
    "\n",
    "        # reset gate\n",
    "        self.kernel_r = self.kernel[:, self.units: self.units * 2]\n",
    "        self.memory_state_kernel_r = self.memory_state_kernel[:,\n",
    "                                                        self.units:\n",
    "                                                        self.units * 2]\n",
    "        self.output_kernel_r = self.output_kernel[:, self.units:self.units * 2]\n",
    "\n",
    "        # new gate convert to cell gate\n",
    "        self.kernel_h = self.kernel[:, self.units * 2:]\n",
    "        self.memory_state_kernel_h = self.memory_state_kernel[:, self.units * 2:\n",
    "                                                              self.units * 3]\n",
    "        self.output_kernel_h = self.output_kernel[:, self.units * 2:]\n",
    "        \n",
    "        self.memory_state_kernel_o = self.memory_state_kernel[:, self.units * 3:]\n",
    "\n",
    "        \n",
    "        if self.use_bias:\n",
    "            # bias for inputs\n",
    "            self.input_bias_z = self.input_bias[:self.units]\n",
    "            self.input_bias_r = self.input_bias[self.units: self.units * 2]\n",
    "            self.input_bias_h = self.input_bias[self.units * 2:]\n",
    "            # bias for hidden state - just for compatibility with CuDNN\n",
    "            if self.memory_state_after:\n",
    "                self.memory_state_bias_z = self.memory_state_bias[:self.units]\n",
    "                self.memory_state_bias_r = (\n",
    "                    self.memory_state_bias[self.units: self.units * 2])\n",
    "                self.memory_state_bias_h = self.memory_state_bias[self.units * 2:]\n",
    "            if self.output_after:\n",
    "                self.output_bias_z = self.output_bias[:self.units]\n",
    "                self.output_bias_r = (\n",
    "                    self.output_bias[self.units: self.units * 2])\n",
    "                self.output_bias_h = self.output_bias[self.units * 2:]\n",
    "            \n",
    "        else:\n",
    "            self.input_bias_z = None\n",
    "            self.input_bias_r = None\n",
    "            self.input_bias_h = None\n",
    "            if self.memory_state_after:\n",
    "                self.memory_state_bias_z = None\n",
    "                self.memory_state_bias_r = None\n",
    "                self.memory_state_bias_h = None\n",
    "            if self.output_after:\n",
    "                self.output_bias_z = None\n",
    "                self.output_bias_r = None\n",
    "                self.output_bias_h = None\n",
    "        self.built = True\n",
    "        \n",
    "        \n",
    "    #-----------------------------------------------------------------------\n",
    "    def call(self, inputs, states, training=None):\n",
    "        o_tm1 = states[0]  # previous memory -> previous memory_state and output\n",
    "\n",
    "        if self.h is None:\n",
    "            self.h = K.zeros( (1, self.units), dtype='float32')\n",
    "        h_tm1 = self.h\n",
    "\n",
    "        if 0 < self.dropout < 1 and self._dropout_mask is None:\n",
    "            self._dropout_mask = _generate_dropout_mask(\n",
    "                K.ones_like(inputs),\n",
    "                self.dropout,\n",
    "                training=training,\n",
    "                count=3)\n",
    "        if (0 < self.memory_state_dropout < 1 and\n",
    "                self._memory_state_dropout_mask is None):\n",
    "            self._memory_state_dropout_mask = _generate_dropout_mask(\n",
    "                K.ones_like(h_tm1),\n",
    "                self.recurrent_dropout,\n",
    "                training=training,\n",
    "                count=3)\n",
    "        if (0 < self.output_dropout < 1 and\n",
    "                self._output_dropout_mask is None):\n",
    "            self._output_dropout_mask = _generate_dropout_mask(\n",
    "                K.ones_like(o_tm1),\n",
    "                self.recurrent_dropout,\n",
    "                training=training,\n",
    "                count=3)\n",
    "\n",
    "        # dropout matrices for input units\n",
    "        dp_mask = self._dropout_mask\n",
    "        # dropout matrices for recurrent units\n",
    "        ms_dp_mask = self._memory_state_dropout_mask\n",
    "        out_dp_mask = self._output_dropout_mask\n",
    "        if self.implementation == 1:\n",
    "            # dropout\n",
    "            if 0. < self.dropout < 1.:\n",
    "                inputs_z = inputs * dp_mask[0]\n",
    "                inputs_r = inputs * dp_mask[1]\n",
    "                inputs_h = inputs * dp_mask[2]\n",
    "            else:\n",
    "                inputs_z = inputs\n",
    "                inputs_r = inputs\n",
    "                inputs_h = inputs\n",
    "            \n",
    "            if 0. < self.memory_state_dropout < 1.:\n",
    "                h_tm1_z = h_tm1 * rec_dp_mask[0]\n",
    "                h_tm1_r = h_tm1 * rec_dp_mask[1]\n",
    "                h_tm1_h = h_tm1 * rec_dp_mask[2]\n",
    "            else:\n",
    "                h_tm1_z = h_tm1\n",
    "                h_tm1_r = h_tm1\n",
    "                h_tm1_h = h_tm1\n",
    "            \n",
    "            if 0. < self.output_dropout < 1.:\n",
    "                o_tm1_z = o_tm1 * rec_dp_mask[0]\n",
    "                o_tm1_r = o_tm1 * rec_dp_mask[1]\n",
    "                o_tm1_h = o_tm1 * rec_dp_mask[2]\n",
    "\n",
    "            else:\n",
    "                o_tm1_z = o_tm1\n",
    "                o_tm1_r = o_tm1\n",
    "                o_tm1_h = o_tm1\n",
    "\n",
    "            # calculate gate\n",
    "            x_z = K.dot(inputs_z, self.kernel_z) # update gate - update\n",
    "            x_r = K.dot(inputs_r, self.kernel_r) # reset gate  - reset\n",
    "            x_h = K.dot(inputs_h, self.kernel_h) # cell gate   - memory state\n",
    "\n",
    "            if self.use_bias:\n",
    "                x_z = K.bias_add(x_z, self.input_bias_z)\n",
    "                x_r = K.bias_add(x_r, self.input_bias_r)\n",
    "                x_h = K.bias_add(x_h, self.input_bias_h)\n",
    "\n",
    "            ms_z = K.dot(h_tm1_z, self.memory_state_kernel_z)\n",
    "            ms_r = K.dot(h_tm1_r, self.memory_state_kernel_r)\n",
    "            if self.memory_state_after and self.use_bias:\n",
    "                ms_z = K.bias_add(ms_z, self.memory_state_bias_z)\n",
    "                ms_r = K.bias_add(ms_r, self.memory_state_bias_r)\n",
    "\n",
    "            ##==============================================\n",
    "            # output 계산\n",
    "            out_z = K.dot(o_tm1_z, self.output_kernel_z)\n",
    "            out_r = K.dot(o_tm1_r, self.output_kernel_r)\n",
    "            out_h = K.dot(o_tm1_h, self.output_kernel_h)\n",
    "            if self.output_after and self.use_bias:\n",
    "                out_z = K.bias_add(out_z, self.output_bias_z)\n",
    "                out_r = K.bias_add(out_r, self.output_bias_r)\n",
    "                out_h = K.bias_add(out_h, self.output_bias_h)\n",
    "\n",
    "            z = self.reset_gate_activation(x_z + ms_z + out_z)\n",
    "            r = self.update_gate_activation(x_r + ms_r + out_r)\n",
    "            \n",
    "            # reset gate applied after/before matrix multiplication\n",
    "            if self.memory_state_after:\n",
    "                ms_h = K.dot(h_tm1_h, self.memory_state_kernel_h)\n",
    "                if self.use_bias:\n",
    "                    ms_h = K.bias_add(ms_h, self.memory_state_bias_h)\n",
    "                ms_h = r * ms_h\n",
    "            else:\n",
    "                ms_h = K.dot(r * h_tm1_h, self.memory_state_kernel_h)\n",
    "            \n",
    "            hh = self.cell_gate_activation(x_h + ms_h + out_h)\n",
    "        else: # 먼저 실행\n",
    "            if 0. < self.dropout < 1.:\n",
    "                inputs *= dp_mask[0]\n",
    "\n",
    "            if 0. < self.memory_state_dropout < 1.:\n",
    "                h_tm1 *= ms_dp_mask[0]\n",
    "\n",
    "            if 0. < self.output_dropout < 1.:\n",
    "                o_tm1 *= out_dp_mask[0]\n",
    "                \n",
    "            # inputs projected by all gate matrices at once\n",
    "            matrix_x = K.dot(inputs, self.kernel)\n",
    "\n",
    "            if self.use_bias:\n",
    "                # biases: bias_z_i, bias_r_i, bias_h_i\n",
    "                matrix_x = K.bias_add(matrix_x, self.input_bias)\n",
    "            x_z = matrix_x[:, :self.units]\n",
    "            x_r = matrix_x[:, self.units: 2 * self.units]\n",
    "            x_h = matrix_x[:, 2 * self.units:]\n",
    "\n",
    "\n",
    "            if self.memory_state_after:\n",
    "                # hidden state projected by all gate matrices at once\n",
    "                matrix_inner = K.dot(h_tm1, self.memory_state_kernel[:, :2 * self.units])\n",
    "                if self.use_bias:\n",
    "                    matrix_inner = K.bias_add(matrix_inner, self.memory_state_bias[:, :2 * self.units])\n",
    "            else:\n",
    "                # hidden state projected separately for update/reset and new\n",
    "                matrix_inner = K.dot(h_tm1,\n",
    "                                     self.memory_state_kernel[:, :2 * self.units])\n",
    "\n",
    "            ms_z = matrix_inner[:, :self.units]\n",
    "            ms_r = matrix_inner[:, self.units: 2 * self.units]\n",
    "\n",
    "            matrix_out = K.dot(o_tm1, self.output_kernel)\n",
    "            if self.use_bias and self.output_after:\n",
    "                # hidden state projected by all gate matrices at once\n",
    "                matrix_out = K.bias_add(matrix_out, self.output_bias)\n",
    "                # hidden state projected separately for update/reset and new\n",
    "            \n",
    "            out_z = matrix_out[:, :self.units]\n",
    "            out_r = matrix_out[:, self.units: 2 * self.units]\n",
    "            out_h = matrix_out[:, 2 * self.units:]\n",
    "            \n",
    "            z = self.reset_gate_activation(x_z + ms_z + out_z)\n",
    "            \n",
    "            r = self.update_gate_activation(x_r + ms_r + out_r)\n",
    "\n",
    "            ms_h = K.dot(r * h_tm1, self.memory_state_kernel_h)\n",
    "            if self.memory_state_after:\n",
    "                ms_h = K.bias_add(ms_h, self.memory_state_bias_h)\n",
    "            \n",
    "            hh = self.cell_gate_activation(x_h + ms_h + out_h)\n",
    "        \n",
    "        # previous and candidate state mixed by update gate\n",
    "        self.h = (1 - z) * h_tm1 + z * hh # new ms\n",
    "        \n",
    "        o = K.clip(K.dot(self.h, self.memory_state_kernel_o), 0, 1)\n",
    "        \n",
    "        if 0 < self.dropout + self.memory_state_dropout:\n",
    "            if training is None:\n",
    "                h._uses_learning_phase = True\n",
    "        \n",
    "        return o, [o]\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'units':self.units,\n",
    "#                   'state_size': self.state_size,\n",
    "#                   'output_size':self.output_size,\n",
    "                  'reset_activation': activations.serialize(self.reset_gate_activation),\n",
    "                  'update_activation':\n",
    "                      activations.serialize(self.update_gate_activation),\n",
    "                  'cell_activation':\n",
    "                      activations.serialize(self.cell_gate_activation),\n",
    "                  'use_bias': self.use_bias,\n",
    "                  'kernel_initializer':\n",
    "                      initializers.serialize(self.kernel_initializer),\n",
    "                  'memory_state_kernel_initializer':\n",
    "                      initializers.serialize(self.memory_state_kernel_initializer),\n",
    "                  'output_kernel_initializer':\n",
    "                      initializers.serialize(self.output_kernel_initializer),\n",
    "                  'bias_initializer': initializers.serialize(self.bias_initializer),\n",
    "                  'kernel_regularizer':\n",
    "                      regularizers.serialize(self.kernel_regularizer),\n",
    "                  'memory_state_kernel_regularizer':\n",
    "                      regularizers.serialize(self.memory_state_kernel_regularizer),\n",
    "                  'output_kernel_regularizer':\n",
    "                      regularizers.serialize(self.output_kernel_regularizer),\n",
    "                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
    "                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n",
    "                  'memory_state_kernel_constraint':\n",
    "                      constraints.serialize(self.memory_state_kernel_constraint),\n",
    "                  'output_kernel_constraint':\n",
    "                      constraints.serialize(self.output_kernel_constraint),\n",
    "                  'bias_constraint': constraints.serialize(self.bias_constraint),\n",
    "                  'dropout': self.dropout,\n",
    "                  'memory_state_dropout':self.memory_state_dropout,\n",
    "                  'output_dropout': self.output_dropout,\n",
    "                  'implementation': self.implementation,\n",
    "                  'memory_state_after': self.memory_state_after,\n",
    "                  'output_after': self.output_after}\n",
    "        base_config = super(ENUCell, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENU(RNN):\n",
    "    @interfaces.legacy_recurrent_support\n",
    "    def __init__(self, units,\n",
    "                 reset_activation='sigmoid',\n",
    "                 update_activation='sigmoid',\n",
    "                 cell_activation='tanh',\n",
    "                 use_bias=True,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 memory_state_kernel_initializer='orthogonal',\n",
    "                 output_kernel_initializer='orthogonal',\n",
    "                 bias_initializer='zeros',\n",
    "                 activity_regularizer=None,\n",
    "                 kernel_regularizer=None,\n",
    "                 memory_state_kernel_regularizer=None,\n",
    "                 output_kernel_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 memory_state_kernel_constraint=None,\n",
    "                 output_kernel_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 dropout=0.,\n",
    "                 memory_state_dropout=0.,\n",
    "                 output_dropout=0.,\n",
    "                 implementation=2,\n",
    "                 return_sequences=False,\n",
    "                 return_state=False,\n",
    "                 go_backwards=False,\n",
    "                 stateful=False,\n",
    "                 unroll=False,\n",
    "                 memory_state_after=False,\n",
    "                 output_after=False,\n",
    "                 **kwargs):\n",
    "        if implementation == 0:\n",
    "            warnings.warn('`implementation=0` has been deprecated, '\n",
    "                          'and now defaults to `implementation=1`.'\n",
    "                          'Please update your layer call.')\n",
    "        if K.backend() == 'theano' and (dropout or memory_state_dropout or output_dropout):\n",
    "            warnings.warn(\n",
    "                'RNN dropout is no longer supported with the Theano backend '\n",
    "                'due to technical limitations. '\n",
    "                'You can either set `dropout` and `recurrent_dropout` to 0, '\n",
    "                'or use the TensorFlow backend.')\n",
    "            dropout = 0.\n",
    "            memory_state_dropout = 0.\n",
    "            output_dropout = 0.\n",
    "\n",
    "        cell = ENUCell(units,\n",
    "                     reset_activation=reset_activation,\n",
    "                     update_activation=update_activation,\n",
    "                     cell_activation=cell_activation,\n",
    "                     use_bias=use_bias,\n",
    "                     kernel_initializer=kernel_initializer,\n",
    "                     memory_state_kernel_initializer=memory_state_kernel_initializer,\n",
    "                     output_kernel_initializer=output_kernel_initializer,\n",
    "                     bias_initializer=bias_initializer,\n",
    "                     kernel_regularizer=kernel_regularizer,\n",
    "                     memory_state_kernel_regularizer=memory_state_kernel_regularizer,\n",
    "                     output_kernel_regularizer=output_kernel_regularizer,\n",
    "                     bias_regularizer=bias_regularizer,\n",
    "                     kernel_constraint=kernel_constraint,\n",
    "                     memory_state_kernel_constraint=memory_state_kernel_constraint,\n",
    "                     output_kernel_constraint=output_kernel_constraint,\n",
    "                     bias_constraint=bias_constraint,\n",
    "                     dropout=dropout,\n",
    "                     memory_state_dropout=memory_state_dropout,\n",
    "                     output_dropout=output_dropout,\n",
    "                     implementation=implementation,)\n",
    "        super(ENU, self).__init__(cell,\n",
    "                                  return_sequences=return_sequences,\n",
    "                                  return_state=return_state,\n",
    "                                  go_backwards=go_backwards,\n",
    "                                  stateful=stateful,\n",
    "                                  unroll=unroll,\n",
    "                                  **kwargs)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "\n",
    "    def call(self, inputs, mask=None, training=None, initial_state=None):\n",
    "        self.cell._dropout_mask = None\n",
    "        self.cell._memory_state_dropout_mask = None\n",
    "        self.cell._output_dropout_mask = None\n",
    "        \n",
    "        return super(ENU, self).call(inputs,\n",
    "                                     mask=mask,\n",
    "                                     training=training,\n",
    "                                     initial_state=initial_state)\n",
    "\n",
    "    @property\n",
    "    def units(self):\n",
    "        return self.cell.units\n",
    "\n",
    "    def state_size(self):\n",
    "        return self.cell.state_size\n",
    "\n",
    "    @property\n",
    "    def reset_activation(self):\n",
    "        return self.cell.reset_activation\n",
    "    \n",
    "    @property\n",
    "    def update_activation(self):\n",
    "        return self.cell.update_activation\n",
    "    \n",
    "    @property\n",
    "    def cell_activation(self):\n",
    "        return self.cell.cell_activation\n",
    "\n",
    "    @property\n",
    "    def use_bias(self):\n",
    "        return self.cell.use_bias\n",
    "\n",
    "    @property\n",
    "    def kernel_initializer(self):\n",
    "        return self.cell.kernel_initializer\n",
    "\n",
    "    @property\n",
    "    def memory_state_kernel_initializer(self):\n",
    "        return self.cell.memory_state_kernel_initializer\n",
    "\n",
    "    @property\n",
    "    def output_kernel_initializer(self):\n",
    "        return self.cell.output_kernel_initializer\n",
    "\n",
    "    @property\n",
    "    def bias_initializer(self):\n",
    "        return self.cell.bias_initializer\n",
    "\n",
    "    @property\n",
    "    def kernel_regularizer(self):\n",
    "        return self.cell.kernel_regularizer\n",
    "\n",
    "    @property\n",
    "    def memory_state_kernel_regularizer(self):\n",
    "        return self.cell.memory_state_kernel_regularizer\n",
    "    \n",
    "    @property\n",
    "    def output_kernel_regularizer(self):\n",
    "        return self.cell.output_kernel_regularizer\n",
    "\n",
    "    @property\n",
    "    def bias_regularizer(self):\n",
    "        return self.cell.bias_regularizer\n",
    "\n",
    "    @property\n",
    "    def kernel_constraint(self):\n",
    "        return self.cell.kernel_constraint\n",
    "\n",
    "    @property\n",
    "    def memory_state_kernel_constraint(self):\n",
    "        return self.cell.memory_state_kernel_constraint\n",
    "    \n",
    "    @property\n",
    "    def output_kernel_constraint(self):\n",
    "        return self.cell.output_kernel_constraint\n",
    "    \n",
    "    @property\n",
    "    def bias_constraint(self):\n",
    "        return self.cell.bias_constraint\n",
    "\n",
    "    @property\n",
    "    def dropout(self):\n",
    "        return self.cell.dropout\n",
    "\n",
    "    @property\n",
    "    def recurrent_dropout(self):\n",
    "        return self.cell.recurrent_dropout\n",
    "\n",
    "    @property\n",
    "    def output_dropout(self):\n",
    "        return self.cell.output_dropout\n",
    "    \n",
    "    @property\n",
    "    def implementation(self):\n",
    "        return self.cell.implementation\n",
    "\n",
    "    @property\n",
    "    def memory_state_after(self):\n",
    "        return self.cell.memory_state_after\n",
    "\n",
    "    @property\n",
    "    def output_after(self):\n",
    "        return self.cell.output_after\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'units': self.units,\n",
    "#                   'output_size': self.cell.output_size,\n",
    "#                   'state_size': self.cell.state_size,\n",
    "                  'reset_activation': activations.serialize(self.cell.reset_gate_activation),\n",
    "                  'update_activation':\n",
    "                      activations.serialize(self.cell.update_gate_activation),\n",
    "                  'cell_activation':\n",
    "                      activations.serialize(self.cell.cell_gate_activation),\n",
    "                  'use_bias': self.use_bias,\n",
    "                  'kernel_initializer':\n",
    "                      initializers.serialize(self.cell.kernel_initializer),\n",
    "                  'memory_state_kernel_initializer':\n",
    "                      initializers.serialize(self.cell.memory_state_kernel_initializer),\n",
    "                  'output_kernel_initializer':\n",
    "                      initializers.serialize(self.cell.output_kernel_initializer),\n",
    "                  'bias_initializer': initializers.serialize(self.cell.bias_initializer),\n",
    "                  'kernel_regularizer':\n",
    "                      regularizers.serialize(self.cell.kernel_regularizer),\n",
    "                  'memory_state_kernel_regularizer':\n",
    "                      regularizers.serialize(self.cell.memory_state_kernel_regularizer),\n",
    "                  'output_kernel_regularizer':\n",
    "                      regularizers.serialize(self.cell.output_kernel_regularizer),\n",
    "                  'bias_regularizer': regularizers.serialize(self.cell.bias_regularizer),\n",
    "                  'activity_regularizer':\n",
    "                      regularizers.serialize(self.activity_regularizer),\n",
    "                  'kernel_constraint': constraints.serialize(self.cell.kernel_constraint),\n",
    "                  'memory_state_kernel_constraint':\n",
    "                      constraints.serialize(self.cell.memory_state_kernel_constraint),\n",
    "                  'output_kernel_constraint':\n",
    "                      constraints.serialize(self.cell.output_kernel_constraint),\n",
    "                  'bias_constraint': constraints.serialize(self.cell.bias_constraint),\n",
    "                  'dropout': self.dropout,\n",
    "                  'memory_state_dropout':self.cell.memory_state_dropout,\n",
    "                  'output_dropout': self.cell.output_dropout,\n",
    "                  'implementation': self.cell.implementation,\n",
    "                  'memory_state_after': self.cell.memory_state_after,\n",
    "                  'output_after': self.cell.output_after}\n",
    "        base_config = super(ENU, self).get_config()\n",
    "        del base_config['cell']\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        if 'implementation' in config and config['implementation'] == 0:\n",
    "            config['implementation'] = 1\n",
    "        return cls(**config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
